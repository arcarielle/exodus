# exodus

RAM Information:

The internal memory controller (iMC) in a Haswell-E proc is designed to address up to 4 channels (and up to 64MB) of DDR4-2133. Quad-channel actually means there's four discrete memory controllers working in parallel, each is tasked to addressing the memory occupying one electrical DIMM. Just like multiple CPU cores work in parallel but (ignoring things like HyperThreading) each can only run a single computing pipeline.

The R5E happens to splice the four electrical DIMMs into two physical slots each, so it supports a total of 8 physical DIMMs (pretty much exactly what Intel designed into X99 and Haswell-E).

Populating half the channels with two DIMMs each forces two of the iMCs to work almost twice as hard while the other two iMCs are idling or tasked to supporting functions, it's less efficient than populating all four DIMMs so the load is balanced across all four iMCs. "Less efficient" referring to overworked iMC circuitry needing a slight voltage bump (and thus running a wee bit hotter) as well as a higher incidence of signalling errors and "traffic jams" on the saturated data bus (which slightly increases overall latency). The iMCs themselves are apparently internally hardwired directly into discrete pairs of CPU cores, and needlessly re-routing tons of electrical signals across your processor die accomplishes little more than turning power into heat. You should want all that data sitting around in your RAM to flow in and out of all your processor cores at the same rate so there's no bottlenecks and no hotspots, less strain on the iMC means more headroom to cool off or overvolt and strain some other piece of silicon, lol.

And then there are Double-Sided DIMMs. As the name implies, these are DIMMs which have memory chips soldered onto both sides of the PCB. The DIMM itself has yet another dedicated memory controller chip which translates all this memory into a single logical DIMM. This unavoidably adds a teeny bit more latency to all memory functions on the DIMM. Server RDIMMs are typically capable of reading and writing different memory segments simultaneously (which, along with greater memory capacity, generally outweighs the disadvantages of added latency), but consumer UDIMMs (used by X99 mobos like the R5E) are "cheap" stuff which generally lack this capability. The sad truth is that UDIMMs are often Double-Sided because they allow manufacturers to easily pack on twice the capacity at same speed and lower cost (bulk low-density memory chips cost a lot less and are easier to bin than bulk high-density memory chips) - so the majority of 32GB+ DDR4 kits these days are made of DS DIMMs.

Ideally, you'd want to populate each of your four DDR4 channels with one Single-Sided DIMM. There's no logical difference between four Single-Sided DIMMs or four Double-Sided DIMMs or eight Single-Sided DIMMs, the processor will still "see" four DIMMs either way, although two double-loaded channels will be worse than four single-loaded channels and eight physical DIMMs will tend to have more complex and slightly slower timings than four physical DIMMs. Any real difference would largely be based entirely on the capabilities of the iMCs built into the X99 platform vs the capabilities of the memory controllers onboard the DIMMs. And, unless you end up with garbage parts, the differences would be insignificant - you might actually be able to isolate them in benchmarks (and they do all add up) but you would never notice any difference in real usage. If you're planning on some hard overclocking then it can't hurt to work within the limits built into all your hardware.

Assuming otherwise identical specs, I would just grab whichever of the two kits costs less or looks best or whatever. Don't bother planning on keeping half your DIMM slots open for future memory upgrades because you can't really mix DDR4 kits anyhow, you'd still have to yank out all of whatever's already installed if you want to increase capacity later.
Server folks might advise the opposite, but they're primarily interested in server platforms which work a little differently (what we call "Double-Sided" they call "Dual-Ranked", and they just love RDIMMs which pack on more and more Ranks because they gain capacity and performance advantages from it, while consumer platforms tend to gain greater memory capacity at the cost of greater latencies which impact performance).
